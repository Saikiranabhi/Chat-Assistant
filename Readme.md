# ğŸ“„ğŸ§  RAG Chat Assistant with Pinecone + LLaMA

A Retrieval-Augmented Generation (RAG) chatbot that allows users to upload PDF documents and interactively ask questions about their content. Built with a **modular architecture** using LangChain, Pinecone, Ollama (LLaMA), and Streamlit.

---

## ğŸš€ Features

- **PDF Upload & Parsing** â€” Upload any PDF and extract its text for analysis
- **Semantic Search** â€” Uses Pinecone vector database with HuggingFace embeddings
- **PDF Isolation** â€” Each PDF stored in its own Pinecone namespace; queries never cross-contaminate
- **Metadata Tracking** â€” Stores pdf_id, name, upload date, chunk info per vector
- **Custom Prompt Template** â€” Structured prompts for accurate, factual answers
- **Source Transparency** â€” Shows which chunks were used to generate the answer
- **Chat History** â€” Tracks Q&A pairs per session
- **Modular Codebase** â€” Clean separation of concerns across multiple modules

---

## ğŸ“ Project Structure

```
rag_modular_project/
â”‚
â”œâ”€â”€ app_modular.py              # Main Streamlit entry point & orchestrator
â”œâ”€â”€ config.py                   # All configurations & environment variables
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Secret keys (NOT committed to git)
â”œâ”€â”€ .env.example                # Template for .env file
â”œâ”€â”€ .gitignore                  # Ignores .env, venv, __pycache__, etc.
â”‚
â”œâ”€â”€ utils/                      # Data processing utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pdf_processor.py        # PDF text extraction & chunking
â”‚   â”œâ”€â”€ embeddings.py           # Embedding model initialization
â”‚   â””â”€â”€ prompts.py              # Prompt templates for LLM
â”‚
â”œâ”€â”€ database/                   # Vector database layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pinecone_manager.py     # Pinecone client & index management
â”‚   â””â”€â”€ vector_store.py         # Store & search vectors with metadata
â”‚
â”œâ”€â”€ retrieval/                  # Retrieval & QA logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ retriever.py            # Document retrieval configuration
â”‚   â””â”€â”€ qa_chain.py             # QA chain using LCEL pipeline
â”‚
â””â”€â”€ frontend/                   # Streamlit UI layer
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ ui_components.py        # Reusable display components
    â””â”€â”€ session_manager.py      # Session state management
```

---

## ğŸ› ï¸ Technologies Used

| Technology | Purpose | Version |
|-----------|---------|---------|
| **Python** | Core language | 3.10+ |
| **Streamlit** | Web UI | Latest |
| **LangChain** | RAG pipeline orchestration | Latest |
| **langchain-core** | LCEL chain building | Latest |
| **langchain-community** | Ollama LLM integration | Latest |
| **langchain-pinecone** | Pinecone vector store | Latest |
| **langchain-text-splitters** | Text chunking | Latest |
| **langchain-huggingface** | HuggingFace embeddings | Latest |
| **Pinecone** | Cloud vector database | Latest |
| **HuggingFace** | Sentence embeddings | BAAI/bge-large-en-v1.5 |
| **Ollama** | Local LLM runner | Latest |
| **LLaMA 3** | Language model for QA | llama3:instruct |
| **PyPDF2** | PDF text extraction | Latest |
| **python-dotenv** | .env file loading | Latest |

---

## ğŸ—ï¸ Architecture

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STREAMLIT WEB UI                          â”‚
â”‚               (frontend/ui_components.py)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚
          â–¼                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  PDF Upload  â”‚         â”‚ Question Input â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â–¼                         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
  â”‚  PyPDF2 Parser   â”‚             â”‚
  â”‚  (extract text)  â”‚             â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
         â”‚                         â”‚
         â–¼                         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
  â”‚  Text Chunking   â”‚             â”‚
  â”‚  chunk_size=1000 â”‚             â”‚
  â”‚  overlap=200     â”‚             â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
         â”‚                         â”‚
         â–¼                         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
  â”‚  BAAI/bge-large  â”‚             â”‚
  â”‚  Embeddings      â”‚             â”‚
  â”‚  (1024 dims)     â”‚             â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
         â”‚                         â”‚
         â–¼                         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚       PINECONE               â”‚ â”‚
  â”‚  Index: chat-assistant       â”‚ â”‚
  â”‚  Namespace: <pdf_id>         â”‚ â”‚
  â”‚  Metric: cosine              â”‚ â”‚
  â”‚  Metadata: pdf_id, name,     â”‚ â”‚
  â”‚  chunk_index, upload_date    â”‚ â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Embed Query     â”‚
          â”‚ Search Pinecone â”‚
          â”‚ Filter: pdf_id  â”‚
          â”‚ Top K=5 chunks  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Prompt Template â”‚
          â”‚ + Context       â”‚
          â”‚ + Question      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ LLaMA 3 Instructâ”‚
          â”‚ (via Ollama)    â”‚
          â”‚ temp=0.2        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Answer + Source â”‚
          â”‚ Documents       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

**Upload Flow:**
```
PDF â†’ Extract Text â†’ Chunk (1000 chars) â†’ Embed (1024 dims) â†’ Pinecone (with metadata)
```

**Query Flow:**
```
Question â†’ Embed â†’ Search Pinecone (filter by pdf_id) â†’ Top 5 Chunks â†’ Prompt â†’ LLaMA â†’ Answer
```

---

## âš¡ Getting Started

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.com/download) installed
- [Pinecone](https://www.pinecone.io) account (free tier works)

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
```

### 2. Create Virtual Environment

```bash
python -m venv venv

# Activate on Windows
venv\Scripts\activate

# Activate on Mac/Linux
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Setup .env File

Create a `.env` file in the project root:

```env
PINECONE_API_KEY=your-pinecone-api-key-here
PINECONE_ENVIRONMENT=us-east-1
```

Get your API key from [pinecone.io](https://www.pinecone.io) â†’ API Keys section.

### 5. Setup Ollama

```bash
# Install Ollama from https://ollama.com/download

# Pull the required model
ollama pull llama3:instruct

# Start Ollama server (keep this running!)
ollama serve
```

### 6. Run the App

Open a **new terminal** (keep Ollama running in the first one):

```bash
streamlit run app_modular.py
```

Visit [http://localhost:8501](http://localhost:8501)

---

## ğŸ“¦ Requirements

```txt
streamlit
PyPDF2
python-dotenv
langchain
langchain-community
langchain-pinecone
langchain-core
langchain-text-splitters
langchain-huggingface
pinecone
sentence-transformers
langsmith
```

---

## âš™ï¸ Configuration (config.py)

All settings are centralized in `config.py`:

```python
# Pinecone
INDEX_NAME = "chat-assistant"
PINECONE_CLOUD = "aws"
PINECONE_ENVIRONMENT = "us-east-1"

# Embeddings
EMBEDDING_MODEL_NAME = "BAAI/bge-large-en-v1.5"
EMBEDDING_DIMENSION = 1024

# Chunking
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# Retrieval
TOP_K_RESULTS = 5
SIMILARITY_METRIC = "cosine"

# LLM
LLM_MODEL = "llama3:instruct"
LLM_TEMPERATURE = 0.2
```

---

## ğŸ“ Example Usage

1. **Start Ollama:** `ollama serve`
2. **Run App:** `streamlit run app_modular.py`
3. **Upload PDF** using the file uploader
4. **Wait** for processing (embedding + Pinecone storage)
5. **Ask questions** like:
   - *"What is the main topic of this document?"*
   - *"Summarize the key findings"*
   - *"What does the document say about X?"*
6. **View source chunks** to verify the answer

---

## ğŸ”§ Common Errors & Fixes

| Error | Fix |
|-------|-----|
| `ModuleNotFoundError: langchain.chains` | Use LCEL pipeline in qa_chain.py |
| `ModuleNotFoundError: langchain.text_splitter` | `from langchain_text_splitters import ...` |
| `ModuleNotFoundError: langchain.prompts` | `from langchain_core.prompts import ...` |
| `ModuleNotFoundError: langchain.schema` | `from langchain_core.retrievers import ...` |
| `pinecone-client` renamed error | Use `pinecone` package instead |
| `Vector dimension mismatch` | Delete index, recreate with correct dimension |
| `Ollama connection refused` | Run `ollama serve` in a separate terminal |
| `get_relevant_documents` error | Replace with `.invoke(query)` |

---

## ğŸ† Design Qualities

### âœ… Scalability
- Pinecone handles millions of vectors with auto-scaling
- Serverless architecture â€” no capacity planning needed
- Namespace isolation supports unlimited PDFs

### âœ… Modularity
- Each module has a single, well-defined responsibility
- Easy to swap components (e.g., different LLM or vector DB)
- Clean imports via `__init__.py` files

### âœ… Data Isolation
- Each PDF stored in its own Pinecone namespace
- Metadata filter ensures queries only retrieve from selected PDF
- No cross-document contamination

### âœ… Maintainability
- Centralized config â€” change settings in one place
- Well-documented functions with docstrings
- Clear data flow through the pipeline

### âœ… Transparency
- Source chunks displayed with every answer
- Metadata shows chunk index, upload date, PDF name
- Easy to verify and fact-check answers

---

## ğŸ”„ Extending the Project

### Swap the LLM
In `config.py`:
```python
LLM_MODEL = "mistral"  # or "gemma", "phi3", etc.
```

### Change Embedding Model
In `config.py`:
```python
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
EMBEDDING_DIMENSION = 384
```
> âš ï¸ Delete and recreate Pinecone index if changing embedding model!

### Adjust Chunking
In `config.py`:
```python
CHUNK_SIZE = 500      # Smaller chunks = more precise
CHUNK_OVERLAP = 100
```

### Retrieve More Context
In `config.py`:
```python
TOP_K_RESULTS = 10   # Retrieve more chunks per query
```

---

## ğŸ™ Acknowledgements

- [LangChain](https://github.com/hwchase17/langchain) â€” RAG pipeline framework
- [Pinecone](https://www.pinecone.io) â€” Cloud vector database
- [Ollama](https://ollama.com) â€” Local LLM runner
- [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) â€” Embedding model
- [Streamlit](https://streamlit.io) â€” Web UI framework

---

## ğŸ“„ License

This project is licensed under the MIT License.
